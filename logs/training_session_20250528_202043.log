================================================================================
CIFAR-100 多模型训练会话开始
================================================================================
开始时间: 2025-05-28 20:20:43
Python版本: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]
================================================================================

[2025-05-28 20:20:43] 计划训练 4 个模型
[2025-05-28 20:20:43] (1/4) 开始训练 resnet_20
[2025-05-28 20:20:43] 参数: epochs=20, batch_size=128, lr=0.1
[2025-05-28 20:33:33] ❌ resnet_20 训练失败
[2025-05-28 20:33:33] 错误信息: /opt/venvs/base/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=all` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
[rank0]:W0528 20:21:20.121000 38523 torch/_logging/_internal.py:1130] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W0528 20:21:23.866000 38523 torch/_inductor/utils.py:1250] [4/0] Not enough SMs to use max_autotune_gemm mode
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/d/BaiduSyncdisk/3rd-year-spring/DL/Deep-Learning-2025-Spring/src/train_accelerate.py", line 288, in <module>
[rank0]:     main() 
[rank0]:     ^^^^^^
[rank0]:   File "/mnt/d/BaiduSyncdisk/3rd-year-spring/DL/Deep-Learning-2025-Spring/src/train_accelerate.py", line 275, in main
[rank0]:     history, best_acc = train_with_accelerate(
[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/mnt/d/BaiduSyncdisk/3rd-year-spring/DL/Deep-Learning-2025-Spring/src/train_accelerate.py", line 197, in train_with_accelerate
[rank0]:     train_loss, train_acc, epoch_time = train_epoch(
[rank0]:                                         ^^^^^^^^^^^^
[rank0]:   File "/mnt/d/BaiduSyncdisk/3rd-year-spring/DL/Deep-Learning-2025-Spring/src/train_accelerate.py", line 87, in train_epoch
[rank0]:     running_loss = accelerator.gather_for_metrics(torch.tensor(running_loss / len(trainloader))).mean().item()
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/accelerator.py", line 2732, in gather_for_metrics
[rank0]:     data = self.gather(input_data)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/accelerator.py", line 2688, in gather
[rank0]:     return gather(tensor)
[rank0]:            ^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/utils/operations.py", line 371, in wrapper
[rank0]:     return function(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/utils/operations.py", line 432, in gather
[rank0]:     return _gpu_gather(tensor)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/utils/operations.py", line 351, in _gpu_gather
[rank0]:     return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
[rank0]:     return func(data, *args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/utils/operations.py", line 341, in _gpu_gather_one
[rank0]:     gather_op(output_tensors, tensor)
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 3836, in all_gather_into_tensor
[rank0]:     work = group._allgather_base(output_tensor, input_tensor, opts)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: ValueError: Tensors must be CUDA and dense
[rank0]:[W528 20:33:17.101011167 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
E0528 20:33:24.337000 38425 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 38523) of binary: /opt/venvs/base/bin/python
Traceback (most recent call last):
  File "/opt/venvs/base/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/commands/launch.py", line 1189, in launch_command
    multi_gpu_launcher(args)
  File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/commands/launch.py", line 815, in multi_gpu_launcher
    distrib_run.run(args)
  File "/opt/venvs/base/lib/python3.12/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/opt/venvs/base/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/venvs/base/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
src/train_accelerate.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-28_20:33:24
  host      : book.
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 38523)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

[2025-05-28 20:33:33] 等待5秒后开始下一个模型训练...
[2025-05-28 20:33:38] (2/4) 开始训练 eca_resnet_20
[2025-05-28 20:33:38] 参数: epochs=20, batch_size=128, lr=0.1
[2025-05-28 20:36:50] ❌ eca_resnet_20 训练失败
[2025-05-28 20:36:50] 错误信息: /opt/venvs/base/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=all` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
[rank0]:W0528 20:34:23.587000 41915 torch/_logging/_internal.py:1130] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W0528 20:34:28.132000 41915 torch/_inductor/utils.py:1250] [4/0] Not enough SMs to use max_autotune_gemm mode
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/d/BaiduSyncdisk/3rd-year-spring/DL/Deep-Learning-2025-Spring/src/train_accelerate.py", line 288, in <module>
[rank0]:     main() 
[rank0]:     ^^^^^^
[rank0]:   File "/mnt/d/BaiduSyncdisk/3rd-year-spring/DL/Deep-Learning-2025-Spring/src/train_accelerate.py", line 275, in main
[rank0]:     history, best_acc = train_with_accelerate(
[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/mnt/d/BaiduSyncdisk/3rd-year-spring/DL/Deep-Learning-2025-Spring/src/train_accelerate.py", line 197, in train_with_accelerate
[rank0]:     train_loss, train_acc, epoch_time = train_epoch(
[rank0]:                                         ^^^^^^^^^^^^
[rank0]:   File "/mnt/d/BaiduSyncdisk/3rd-year-spring/DL/Deep-Learning-2025-Spring/src/train_accelerate.py", line 87, in train_epoch
[rank0]:     running_loss = accelerator.gather_for_metrics(torch.tensor(running_loss / len(trainloader))).mean().item()
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/accelerator.py", line 2732, in gather_for_metrics
[rank0]:     data = self.gather(input_data)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/accelerator.py", line 2688, in gather
[rank0]:     return gather(tensor)
[rank0]:            ^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/utils/operations.py", line 371, in wrapper
[rank0]:     return function(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/utils/operations.py", line 432, in gather
[rank0]:     return _gpu_gather(tensor)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/utils/operations.py", line 351, in _gpu_gather
[rank0]:     return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
[rank0]:     return func(data, *args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/utils/operations.py", line 341, in _gpu_gather_one
[rank0]:     gather_op(output_tensors, tensor)
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/venvs/base/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 3836, in all_gather_into_tensor
[rank0]:     work = group._allgather_base(output_tensor, input_tensor, opts)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: ValueError: Tensors must be CUDA and dense
[rank0]:[W528 20:36:34.834430126 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
E0528 20:36:40.360000 41795 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 41915) of binary: /opt/venvs/base/bin/python
Traceback (most recent call last):
  File "/opt/venvs/base/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/commands/launch.py", line 1189, in launch_command
    multi_gpu_launcher(args)
  File "/opt/venvs/base/lib/python3.12/site-packages/accelerate/commands/launch.py", line 815, in multi_gpu_launcher
    distrib_run.run(args)
  File "/opt/venvs/base/lib/python3.12/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/opt/venvs/base/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/venvs/base/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
src/train_accelerate.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-28_20:36:40
  host      : book.
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 41915)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

[2025-05-28 20:36:50] 等待5秒后开始下一个模型训练...
[2025-05-28 20:36:55] (3/4) 开始训练 ghost_resnet_20
[2025-05-28 20:36:55] 参数: epochs=20, batch_size=128, lr=0.1
