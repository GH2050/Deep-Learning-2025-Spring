# 基于ResNet骨干网络利用先进卷积结构与注意力机制增强CIFAR-100分类性能 - 演示文稿大纲

**演示时长**: 8-10分钟
**团队**: 董瑞昕、廖望、卢艺晗、谭凯泽、喻心

---

##幻灯片 1: 标题页 (约30秒)

-   **标题**: 基于ResNet骨干网络利用先进卷积结构与注意力机制增强CIFAR-100分类性能
-   **副标题**: DL-2025课程项目报告
-   **团队成员**: 董瑞昕、廖望、卢艺晗、谭凯泽、喻心
-   **日期**: 2025年06月10日
-   **Logo**: (学校/课程Logo)

---

## 幻灯片 2: 项目概述与目标 (约1分钟)

-   **项目背景**: 
    *   CIFAR-100分类任务挑战
    *   先进深度学习架构与注意力机制的重要性
-   **核心目标**:
    1.  实现并对比10种先进方法 (基于ResNet)
    2.  分析性能、效率与模块有效性
    3.  （复用已有成果，重点在于框架搭建与分析）
-   **技术栈**: PyTorch, Accelerate, timm, Transformers

---

## 幻灯片 3: 相关工作 - 十种先进方法概览 (约1.5分钟)

-   **简要介绍ResNet基础架构**
-   **十种先进方法 Logo墙/列表**: (每个方法一句话核心思想)
    1.  **ConvNeXt**: 现代化卷积网络
    2.  **SegNeXt (MSCA)**: 多尺度卷积注意力
    3.  **LSKNet**: 大型选择性核 (概念引入)
    4.  **CoatNet**: 卷积与注意力融合
    5.  **ECA-Net**: 高效通道注意力
    6.  **CSPNet**: 跨阶段局部网络
    7.  **GhostNet**: 廉价操作生成特征
    8.  **HorNet**: 递归门控卷积
    9.  **ResNeSt**: 分裂注意力
    10. **MLP-Mixer**: 纯MLP架构
-   **强调本项目实现了17个模型变体覆盖这些方法**

---

## 幻灯片 4: 方法设计 - 统一框架与实验流程 (约1分钟)

-   **统一模型注册 (`MODEL_REGISTRY`)**
-   **自动化实验流程**: 
    *   数据生成 (`generate_results.py`)
    *   对比实验 (`comparison_experiments.py`)
    *   消融实验 (`ablation_experiments.py`)
    *   结果分析与可视化 (`analyze_results.py`)
-   **数据预处理**: CIFAR-100标准化, ImageNet预训练适配
-   **训练设置**: SGD, CosineAnnealingLR, 15 Epochs (模拟)

---

## 幻灯片 5: 核心实验结果 - 整体性能对比 (约1.5分钟)

-   **展示核心性能表 (Top-1, Top-5, 参数量, 时间)** (引用表1)
    *   突出最佳模型: `ghostnet_100` (75.71%), `convnext_tiny_timm` (74.79%)
-   **图表**: 模型Top-1准确率柱状图 (引用图1)
    *   解读图表，指出高性能模型和基线对比
-   **关键信息**: 预训练的重要性，不同架构的性能表现差异

---

## 幻灯片 6: 核心实验结果 - 效率分析 (约1分钟)

-   **图表**: 参数效率散点图 (准确率 vs. 参数量) (引用图2)
    *   解读: `ghost_resnet_20` (1621), `segnext_mscan_tiny` (71.68)
-   **讨论**: 
    *   参数效率最高的模型 (`ghost_resnet_20`)
    *   训练速度最快的模型 (`ghost_resnet_20`)
    *   准确率-参数-速度的权衡

---

## 幻灯片 7: 消融实验结果与分析 (约1.5分钟)

-   **ECA-Net消融**: 
    *   Baseline vs. ECA vs. ECA (k=3) (引用表格)
    *   结论: ECA有效性 (+3.35%), 自适应核更优
-   **GhostNet消融**:
    *   Baseline vs. Ghost (ratio 2 & 4) (引用表格)
    *   结论: 参数大幅减少 (高达91%), 准确率有所牺牲但效率提升
-   **注意力位置消融**:
    *   Baseline vs. ECA (前/后) (引用表格)
    *   结论: 残差前应用更佳 (+2.9%)
-   **核心价值**: 验证了关键设计和模块的有效性

---

## 幻灯片 8: 关键发现与总结 (约1分钟)

-   **性能关键发现**:
    *   预训练 > 从头训练
    *   GhostNet平衡性好，混合架构潜力大
    *   注意力机制普遍有效
-   **效率关键发现**:
    *   Ghost模块极致轻量
    *   不同架构训练开销差异大
-   **项目创新点**: 统一框架, 自动化流程, 动态归一化

---

## 幻灯片 9: 团队贡献 (约30秒)

-   **清晰列出5位成员及其核心贡献** (根据报告第9节)
    *   董瑞昕: 技术负责, 核心架构, 分析脚本
    *   廖望: 先进CNN/混合模型集成
    *   卢艺晗: 新兴MLP/SegNeXt架构探索
    *   谭凯泽: 消融实验设计与实现
    *   喻心: 报告整合与文档管理
-   **强调团队合作的重要性**

---

## 幻灯片 10: 未来工作与展望 (约30秒)

-   **真实训练与超参数调优**
-   **探索更前沿技术 (ViT, NAS, 知识蒸馏)**
-   **深入分析与可解释性**
-   **工程优化与部署**
-   **感谢语 & Q&A**

---

## 辅助幻灯片 (可选, Q&A备用)

-   详细的技术栈与环境配置
-   `requirement.md`核心要求回顾
-   LSKNet等未能充分实验的方法的简要讨论
-   更详细的训练曲线或数据表格 