# 基于ResNet骨干网络利用先进卷积结构与注意力机制增强CIFAR-100分类性能 - 演示文稿大纲

**演示时长**: 8-10分钟
**汇报人**: 廖望

---

##幻灯片 1: 标题页 (约30秒)

-   **标题**: 基于ResNet骨干网络利用先进卷积结构与注意力机制增强CIFAR-100分类性能
-   **副标题**: DL-2025课程项目报告
-   **汇报人**: 廖望
-   **(团队成员**: 董瑞昕、卢艺晗、谭凯泽、喻心 - 本项目由团队合作完成，详情将在致谢部分说明)
-   **日期**: 2025年06月10日
-   **Logo**: (学校/课程Logo)

---

## 幻灯片 2: 项目概述与目标 (约1分钟)

-   **项目背景**:
    *   CIFAR-100分类任务挑战
    *   先进深度学习架构与注意力机制的重要性
-   **我承担的核心目标**:
    1.  实现并对比10种先进方法 (基于ResNet)
    2.  分析性能、效率与模块有效性 (基于8xV100 GPU实际训练结果)
    3.  （复用已有成果，重点在于框架搭建、实际运行与分析）
-   **技术栈**: PyTorch, Accelerate, timm, Transformers

---

## 幻灯片 3: 相关工作 - 十种先进方法概览 (约1.5分钟)

-   **简要介绍ResNet基础架构**
-   **我们研究和实现的十种先进方法**: (每个方法一句话核心思想)
    1.  **ConvNeXt**: 现代化卷积网络
    2.  **SegNeXt (MSCA)**: 多尺度卷积注意力
    3.  **LSKNet**: 大型选择性核 (概念引入)
    4.  **CoatNet**: 卷积与注意力融合
    5.  **ECA-Net**: 高效通道注意力
    6.  **CSPNet**: 跨阶段局部网络
    7.  **GhostNet**: 廉价操作生成特征
    8.  **HorNet**: 递归门控卷积
    9.  **ResNeSt**: 分裂注意力
    10. **MLP-Mixer**: 纯MLP架构
-   **强调本项目共实现了17个模型变体覆盖这些方法**

---

## 幻灯片 4: 方法设计 - 我构建的统一框架与实验流程 (约1分钟)

-   **统一模型注册 (`MODEL_REGISTRY`)**
-   **自动化实验流程**:
    *   实验数据记录与管理
    *   对比实验 (`run_experiments.py --mode comparison`)
    *   消融实验 (`run_experiments.py --mode ablation`)
    *   结果分析与可视化 (`analyze_results.py`)
-   **数据预处理**: CIFAR-100标准化, ImageNet预训练适配
-   **训练设置**: 8x NVIDIA V100 (16GB), DDP, SGD, CosineAnnealingLR, 200 Epochs, BS/GPU=128 (Total BS=1024)

---

## 幻灯片 5: 核心实验结果 - 整体性能对比 (约1.5分钟)

-   **展示核心性能表 (Top-1, Top-5, 参数量, 训练时间(h))** (引用报告表1)
    *   突出最佳模型: `ghostnet_100` (75.71%), `convnext_tiny_timm` (74.79%)
-   **图表**: 模型Top-1准确率柱状图 (引用报告图1)
    *   解读图表，指出高性能模型和基线对比
-   **关键信息**: 预训练的重要性，不同架构的性能表现差异，所有结果基于我们团队的实际训练。

---

## 幻灯片 6: 核心实验结果 - 效率分析 (约1分钟)

-   **图表**: 参数效率散点图 (准确率 vs. 参数量) (引用报告图2)
    *   解读: `ghost_resnet_20` (1621), `segnext_mscan_tiny` (71.68)
-   **讨论**:
    *   参数效率最高的模型 (`ghost_resnet_20`)
    *   训练速度最快的模型 (`ghost_resnet_20` - 约1小时/200轮/8卡V100)
    *   准确率-参数-速度的权衡

---

## 幻灯片 7: 消融实验结果与分析 (约1.5分钟)

-   **ECA-Net消融**:
    *   Baseline vs. ECA vs. ECA (k=3) (引用报告表格)
    *   结论: ECA有效性 (+3.35%), 自适应核更优
-   **GhostNet消融**:
    *   Baseline vs. Ghost (ratio 2 & 4) (引用报告表格)
    *   结论: 参数大幅减少 (高达91%), 准确率有所牺牲但效率提升
-   **注意力位置消融**:
    *   Baseline vs. ECA (前/后) (引用报告表格)
    *   结论: 残差前应用更佳 (+2.9%)
-   **核心价值**: 这些实验验证了关键设计和模块的有效性 (结果基于我们团队实际运行的消融实验)

---

## 幻灯片 8: 关键发现与总结 (约1分钟)

-   **性能关键发现**:
    *   预训练 > 从头训练
    *   GhostNet平衡性好，混合架构潜力大
    *   注意力机制普遍有效
-   **效率关键发现**:
    *   Ghost模块极致轻量 (训练仅需~1h/8卡V100)
    *   不同架构训练开销差异大 (1h - 7h)
-   **项目主要创新点**: 统一框架, 自动化流程, 动态归一化, 多GPU适配

---

## 幻灯片 9: 项目贡献致谢 (约30秒)

-   **汇报人**: 廖望 - 作为项目核心贡献者，我主导了GhostNet及ConvNeXt系列模型的实现、集成与8卡V100环境下的分布式训练优化，对性能验证与技术攻关有突出贡献，并负责本次汇报。
-   **本项目由以下团队成员共同完成，感谢大家的努力与协作**:
    *   董瑞昕: 技术选型与指导, 核心架构设计, 训练/分析脚本框架, 多GPU适配方案。
    *   卢艺晗: SegNeXt (MSCAN), HorNet模型实现与多GPU适配，参与结果分析与讨论。
    *   谭凯泽: 消融实验方案设计、具体执行与数据整理，确保相关结论的可靠性。
    *   喻心: CoAtNet, CSPNet, ResNeSt, MLP-Mixer模型实现与集成，协助报告撰写与数据整理。

---

## 幻灯片 10: 未来工作与展望 (约30秒)

-   **精细化超参数调优** (AutoAugment, Mixup等)
-   **探索更前沿技术 (ViT, NAS, 知识蒸馏)**
-   **深入分析与可解释性**
-   **工程优化与部署**
-   **感谢各位老师和同学！Q&A** (可再次感谢团队成员、老师和学校提供的资源)

---

## 辅助幻灯片 (可选, Q&A备用)

-   详细的硬件与软件环境 (8xV100服务器配置)
-   `requirement.md`核心要求回顾
-   LSKNet等未能充分实验的方法的简要讨论
-   更详细的训练曲线或数据表格 (基于200轮训练) 